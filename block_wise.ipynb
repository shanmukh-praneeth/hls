{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f76d83e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-09T20:45:14.115106Z",
     "iopub.status.busy": "2025-10-09T20:45:14.114837Z"
    },
    "papermill": {
     "duration": 101.60566,
     "end_time": "2025-10-09T20:46:55.716527",
     "exception": false,
     "start_time": "2025-10-09T20:45:14.110867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 20:45:15.742317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760042715.975116      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760042716.032767      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "I0000 00:00:1760042730.551511      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying block-wise training schedule for Z=16\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# GPU settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# get the base graph and generator matrix\n",
    "code_PCM0 = np.loadtxt(\"/kaggle/input/bg-graph/BaseGraph1_Set0.txt\", int, delimiter='\t')\n",
    "code_PCM1 = np.loadtxt(\"/kaggle/input/bg-graph/BaseGraph1_Set1.txt\", int, delimiter='\t')\n",
    "code_PCM2 = np.loadtxt(\"/kaggle/input/bg-graph/BaseGraph1_Set2.txt\", int, delimiter='\t')\n",
    "code_GM_16 = np.loadtxt(\"/kaggle/input/ldpc-16/LDPC_GM_BG2_16.txt\", int, delimiter=',')\n",
    "# code_GM_3 = np.loadtxt(\"./BaseGraph_GM/LDPC_GM_BG2_3.txt\", int, delimiter=',')\n",
    "# code_GM_10 = np.loadtxt(\"./BaseGraph_GM/LDPC_GM_BG2_10.txt\", int, delimiter=',')\n",
    "# code_GM_6 = np.loadtxt(\"./BaseGraph_GM/LDPC_GM_BG2_6.txt\", int, delimiter=',')\n",
    "code_PCM = code_PCM0.copy()\n",
    "Ldpc_PCM = [code_PCM0, code_PCM1, code_PCM2, code_PCM1]# four LDPC codes with different code lengths\n",
    "Ldpc_GM = [code_GM_16]\n",
    "Z_array = np.array([16, 3, 10, 6])\n",
    "\n",
    "N = code_PCM.shape[1]     # instead of N = 52\n",
    "m = code_PCM.shape[0]\n",
    "code_n = N\n",
    "code_k = N - m\n",
    "\n",
    "code_n = N\n",
    "code_k = N - m\n",
    "for i in range(0, code_PCM.shape[0]):\n",
    "    for j in range(0, code_PCM.shape[1]):\n",
    "        if (code_PCM[i, j] == -1):\n",
    "            code_PCM[i, j] = 0\n",
    "        else:\n",
    "            code_PCM[i, j] = 1\n",
    "\n",
    "# network hyper-parameters\n",
    "iters_max = 25     # number of iterations\n",
    "sum_edge_c = np.sum(code_PCM, axis=1)\n",
    "sum_edge_v = np.sum(code_PCM, axis=0)\n",
    "sum_edge = np.sum(sum_edge_v)\n",
    "neurons_per_even_layer = neurons_per_odd_layer = np.sum(sum_edge_v)\n",
    "input_output_layer_size = N\n",
    "\n",
    "# init the AWGN #\n",
    "code_rate = 1.0 * (N - m) / (N-2)\n",
    "# train SNR\n",
    "SNR_Matrix = np.array([[9.0,6.05,4.1,2.95,2.25,1.8,1.55,1.3,1.15,1.05,0.94,0.85,0.83,0.81,0.8,0.8,0.8,0.75,0.75,0.7,0.7,0.7,0.7,0.7,0.7],\n",
    "                       [9.1,6.2,4.6,3.7,3.2,3.0,2.8,2.7,2.6,2.55,2.5,2.45,2.4,2.4,2.4,2.35,2.35,2.3,2.3,2.3,2.25,2.25,2.25,2.25,2.25],\n",
    "                       [9,6.05,4.1,3,2.4,2,1.7,1.5,1.4,1.4,1.3,1.3,1.2,1.2,1.2,1.2,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1,1],\n",
    "                       [9.0,6.1,4.25,3.2,2.6,2.25,2,1.9,1.8,1.7,1.7,1.65,1.6,1.6,1.55,1.55,1.5,1.5,1.5,1.45,1.45,1.4,1.4,1.4,1.4]])\n",
    "SNR_lin = 10.0 ** (SNR_Matrix / 10.0)\n",
    "SNR_sigma = np.sqrt(1.0 / (2.0 * SNR_lin * code_rate))\n",
    "# ramdom seed\n",
    "word_seed = 2042\n",
    "noise_seed = 1074\n",
    "wordRandom = np.random.RandomState(word_seed)  # word seed\n",
    "noiseRandom = np.random.RandomState(noise_seed)  # noise seed\n",
    "\n",
    "# train settings\n",
    "learning_rate = 0.001\n",
    "train_on_zero_word = True\n",
    "numOfWordSim_train = 50\n",
    "batch_size = numOfWordSim_train\n",
    "num_of_batch = 50000\n",
    "\n",
    "#get train samples\n",
    "def create_mix_epoch(scaling_factor, wordRandom, noiseRandom, numOfWordSim, code_n, code_k, Z, code_GM, is_zeros_word):\n",
    "    X = np.zeros([1, code_n * Z], dtype=np.float32)\n",
    "    Y = np.zeros([1, code_n * Z], dtype=np.int64)\n",
    "\n",
    "    # build set for epoch\n",
    "    for sf_i in scaling_factor:\n",
    "        if is_zeros_word:\n",
    "            infoWord_i = 0 * wordRandom.randint(0, 2, size=(numOfWordSim, code_k * Z))\n",
    "        else:\n",
    "            infoWord_i = wordRandom.randint(0, 2, size=(numOfWordSim, code_k * Z))\n",
    "\n",
    "        Y_i = np.dot(infoWord_i, code_GM) % 2\n",
    "        X_p_i = noiseRandom.normal(0.0, 1.0, Y_i.shape) * sf_i + (-1) ** (1 - Y_i)  # pay attention to this 1->1 0->-1\n",
    "        x_llr_i = 2 * X_p_i / ((sf_i) ** 2)  # defined as p1/p0\n",
    "        X = np.vstack((X, x_llr_i))\n",
    "        Y = np.vstack((Y, Y_i))\n",
    "    X = X[1:]\n",
    "    Y = Y[1:]\n",
    "    X = np.reshape(X, [batch_size, code_n, Z])\n",
    "    return X, Y\n",
    "\n",
    "# calculate ber and fer\n",
    "def calc_ber_fer(snr_db, Y_test_pred, Y_test, numOfWordSim):\n",
    "    ber_test = np.zeros(snr_db.shape[0])\n",
    "    fer_test = np.zeros(snr_db.shape[0])\n",
    "    for i in range(0, snr_db.shape[0]):\n",
    "        Y_test_pred_i = Y_test_pred[i * numOfWordSim:(i + 1) * numOfWordSim, :]\n",
    "        Y_test_i = Y_test[i * numOfWordSim:(i + 1) * numOfWordSim, :]\n",
    "        ber_test[i] = np.abs(((Y_test_pred_i > 0) - Y_test_i)).sum() / (Y_test_i.shape[0] * Y_test_i.shape[1])\n",
    "        fer_test[i] = (np.abs(((Y_test_pred_i > 0) - Y_test_i)).sum(axis=1) > 0).sum() * 1.0 / Y_test_i.shape[0]\n",
    "    return ber_test, fer_test\n",
    "\n",
    "\n",
    "############################     init the connecting matrix between network layers   #################################\n",
    "Lift_Matrix1 = []\n",
    "Lift_Matrix2 = []\n",
    "W_odd2even = np.zeros((sum_edge, sum_edge), dtype=np.float32)\n",
    "W_skipconn2even = np.zeros((N, sum_edge), dtype=np.float32)\n",
    "W_even2odd = np.zeros((sum_edge, sum_edge), dtype=np.float32)\n",
    "W_output = np.zeros((sum_edge, N), dtype=np.float32)\n",
    "\n",
    "# init lifting matrix for cyclic shift\n",
    "for t in range(0, 4, 1):\n",
    "    Lift_M1 = np.zeros((neurons_per_odd_layer * Z_array[t], neurons_per_odd_layer * Z_array[t]), np.float32)\n",
    "    Lift_M2 = np.zeros((neurons_per_odd_layer * Z_array[t], neurons_per_odd_layer * Z_array[t]), np.float32)\n",
    "    code_PCM1 = Ldpc_PCM[t]\n",
    "    k = 0\n",
    "    for j in range(0, code_PCM1.shape[1]):\n",
    "        for i in range(0, code_PCM1.shape[0]):\n",
    "            if (code_PCM1[i, j] != -1):\n",
    "                Lift_num = code_PCM1[i, j] % Z_array[t]\n",
    "                for h in range(0, Z_array[t], 1):\n",
    "                    Lift_M1[k * Z_array[t] + h, k * Z_array[t] + (h + Lift_num) % Z_array[t]] = 1\n",
    "                k = k + 1\n",
    "    k = 0\n",
    "    for i in range(0, code_PCM1.shape[0]):\n",
    "        for j in range(0, code_PCM1.shape[1]):\n",
    "            if (code_PCM1[i, j] != -1):\n",
    "                Lift_num = code_PCM1[i, j] % Z_array[t]\n",
    "                for h in range(0, Z_array[t], 1):\n",
    "                    Lift_M2[k * Z_array[t] + h, k * Z_array[t] + (h + Lift_num) % Z_array[t]] = 1\n",
    "                k = k + 1\n",
    "    Lift_Matrix1.append(Lift_M1)\n",
    "    Lift_Matrix2.append(Lift_M2)\n",
    "\n",
    "# init W_odd2even  variable node updating\n",
    "k = 0\n",
    "vec_tmp = np.zeros((sum_edge), dtype=np.float32)  # even layer index read with column\n",
    "for j in range(0, code_PCM.shape[1], 1):  # run over the columns\n",
    "    for i in range(0, code_PCM.shape[0], 1):  # break after the first one\n",
    "        if (code_PCM[i, j] == 1):  # finding the first one is ok\n",
    "            num_of_conn = int(np.sum(code_PCM[:, j]))  # get the number of connection of the variable node\n",
    "            idx = np.argwhere(code_PCM[:, j] == 1)  # get the indexes\n",
    "            for l in range(0, num_of_conn, 1):  # adding num_of_conn columns to W\n",
    "                vec_tmp = np.zeros((sum_edge), dtype=np.float32)\n",
    "                for r in range(0, code_PCM.shape[0], 1):  # adding one to the right place\n",
    "                    if (code_PCM[r, j] == 1 and idx[l][0] != r):\n",
    "                        idx_row = np.cumsum(code_PCM[r, 0:j + 1])[-1] - 1\n",
    "                        odd_layer_node_count = 0\n",
    "                        if r > 0:\n",
    "                            odd_layer_node_count = np.cumsum(sum_edge_c[0:r])[-1]\n",
    "                        vec_tmp[idx_row + odd_layer_node_count] = 1  # offset index adding\n",
    "                W_odd2even[:, k] = vec_tmp.transpose()\n",
    "                k += 1\n",
    "            break\n",
    "\n",
    "# init W_even2odd  parity check node updating\n",
    "# ----------------------- Fix connection matrices ----------------------- #\n",
    "\n",
    "# init W_even2odd  parity check node updating\n",
    "k = 0\n",
    "for j in range(code_PCM.shape[1]):  # loop over columns (variable nodes)\n",
    "    for i in range(code_PCM.shape[0]):  # loop over rows (check nodes)\n",
    "        if code_PCM[i, j] == 1:\n",
    "            idx_row = np.cumsum(code_PCM[i, 0:j + 1])[-1] - 1\n",
    "            idx_col = np.cumsum(code_PCM[0:i + 1, j])[-1] - 1\n",
    "            odd_layer_node_count_1 = np.sum(sum_edge_c[:i]) if i > 0 else 0\n",
    "            odd_layer_node_count_2 = np.sum(sum_edge_c[:i + 1])\n",
    "\n",
    "            if k < sum_edge:\n",
    "                W_even2odd[k, odd_layer_node_count_1:odd_layer_node_count_2] = 1.0\n",
    "                W_even2odd[k, odd_layer_node_count_1 + idx_row] = 0.0\n",
    "                k += 1  # increment per edge\n",
    "\n",
    "# init W_output (odd to output)\n",
    "k = 0\n",
    "for j in range(code_PCM.shape[1]):\n",
    "    for i in range(code_PCM.shape[0]):\n",
    "        if code_PCM[i, j] == 1:\n",
    "            idx_row = np.cumsum(code_PCM[i, 0:j + 1])[-1] - 1\n",
    "            odd_layer_node_count = np.sum(sum_edge_c[:i]) if i > 0 else 0\n",
    "            if (odd_layer_node_count + idx_row) < sum_edge and k < N:\n",
    "                W_output[odd_layer_node_count + idx_row, k] = 1.0\n",
    "            k += 1  # increment per connection (safe)\n",
    "\n",
    "# init W_skipconn2even (channel input)\n",
    "k = 0\n",
    "for j in range(code_PCM.shape[1]):\n",
    "    for i in range(code_PCM.shape[0]):\n",
    "        if code_PCM[i, j] == 1 and k < sum_edge:\n",
    "            W_skipconn2even[j, k] = 1.0\n",
    "            k += 1\n",
    "\n",
    "\n",
    "##############################  bulid four neural networks(Z = 16,3, 10, 6) ############################\n",
    "net_dict = {}\n",
    "# init the learnable network parameters\n",
    "Weights_Var = 0.5 * np.ones(sum_edge, dtype=np.float32)\n",
    "Biases_Var = np.zeros(sum_edge, dtype=np.float32)\n",
    "for i in range(0, iters_max, 1):\n",
    "    net_dict[\"Weights_Var{0}\".format(i)] = tf.Variable(Weights_Var.copy(), name=\"Weights_Var\".format(i))\n",
    "    net_dict[\"Biases_Var{0}\".format(i)] = tf.Variable(Biases_Var.copy(), name=\"Biases_Var\".format(i))\n",
    "\n",
    "# the decoding neural network of Z=16\n",
    "# ----------------------  Z = 16 NETWORK  ---------------------- #\n",
    "Z = 16\n",
    "xa = tf.placeholder(tf.float32, shape=[batch_size, N, Z], name='xa')\n",
    "ya = tf.placeholder(tf.float32, shape=[batch_size, N * Z], name='ya')\n",
    "xa_input = tf.transpose(xa, [0, 2, 1])\n",
    "net_dict[\"LLRa{0}\".format(0)] = tf.zeros((batch_size, Z, sum_edge), dtype=tf.float32)\n",
    "\n",
    "for i in range(0, iters_max, 1):\n",
    "    # Variable node update\n",
    "    x0 = tf.matmul(xa_input, W_skipconn2even)\n",
    "    x1 = tf.matmul(net_dict[\"LLRa{0}\".format(i)], W_odd2even)\n",
    "    x2 = tf.add(x0, x1)\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer * Z])\n",
    "    x2 = tf.matmul(x2, Lift_Matrix1[0].transpose())\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x_tile = tf.tile(x2, multiples=[1, 1, neurons_per_odd_layer])\n",
    "    W_input_reshape = tf.reshape(W_even2odd.transpose(), [-1])\n",
    "\n",
    "    # Check node update\n",
    "    x_tile_mul = tf.multiply(x_tile, W_input_reshape)\n",
    "    x2_1 = tf.reshape(x_tile_mul, [batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer])\n",
    "    x2_abs = tf.add(tf.abs(x2_1), 10000 * (1 - tf.to_float(tf.abs(x2_1) > 0)))\n",
    "    x3 = tf.reduce_min(x2_abs, axis=3)\n",
    "    x2_2 = -x2_1\n",
    "    x4 = tf.add(tf.zeros((batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer)), \n",
    "                1 - 2 * tf.to_float(x2_2 < 0))\n",
    "    x4_prod = -tf.reduce_prod(x4, axis=3)\n",
    "    x_output_0 = tf.multiply(x3, tf.sign(x4_prod))\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, Z * neurons_per_odd_layer])\n",
    "    x_output_0 = tf.matmul(x_output_0, Lift_Matrix2[0])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "\n",
    "    # Add learnable parameters\n",
    "    x_output_1 = tf.add(\n",
    "        tf.multiply(tf.abs(x_output_0), net_dict[\"Weights_Var{0}\".format(i)]),\n",
    "        net_dict[\"Biases_Var{0}\".format(i)]\n",
    "    )\n",
    "    x_output_1 = tf.multiply(x_output_1, tf.to_float(x_output_1 > 0))\n",
    "    net_dict[\"LLRa{0}\".format(i+1)] = tf.multiply(x_output_1, tf.sign(x_output_0))  # update LLR\n",
    "\n",
    "    # Output\n",
    "    y_output_2 = tf.matmul(net_dict[\"LLRa{0}\".format(i+1)], W_output)\n",
    "    y_output_3 = tf.transpose(y_output_2, [0, 2, 1])\n",
    "    y_output_4 = tf.add(xa, y_output_3)\n",
    "    net_dict[\"ya_output{0}\".format(i)] = tf.reshape(y_output_4, [batch_size, N * Z], name='ya_output'.format(i))\n",
    "\n",
    "    # Loss\n",
    "    net_dict[\"lossa{0}\".format(i)] = 1.0 * tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=ya, logits=net_dict[\"ya_output{0}\".format(i)]\n",
    "        ), name='lossa'.format(i)\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Apply block-wise training schedule (Boosted Neural Decoder style)\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"Applying block-wise training schedule for Z=16\")\n",
    "Δ1 = 5   # block size\n",
    "Δ2 = 10  # retraining window\n",
    "total_iters = iters_max\n",
    "\n",
    "for start_iter in range(0, total_iters, Δ1):\n",
    "    end_iter = min(start_iter + Δ1, total_iters)\n",
    "    train_indices = list(range(max(0, start_iter - Δ2), end_iter))\n",
    "\n",
    "    # Combined loss for Δ₁ + Δ₂ iterations\n",
    "    block_loss = tf.add_n([net_dict[f\"lossa{i}\"] for i in train_indices])\n",
    "\n",
    "    # Collect corresponding learnable vars\n",
    "    block_vars = []\n",
    "    for i in train_indices:\n",
    "        block_vars.extend([\n",
    "            net_dict[f\"Weights_Var{i}\"],\n",
    "            net_dict[f\"Biases_Var{i}\"]\n",
    "        ])\n",
    "\n",
    "    # Define Adam optimizer for this block\n",
    "    block_train_step = tf.train.AdamOptimizer(learning_rate=learning_rate)\\\n",
    "                               .minimize(block_loss, var_list=block_vars)\n",
    "\n",
    "    # Assign this optimizer handle to all iterations in the block\n",
    "    for i in train_indices:\n",
    "        net_dict[f\"train_stepa{i}\"] = block_train_step\n",
    "\n",
    "# the decoding neural network of Z=3\n",
    "Z = 3\n",
    "xb = tf.placeholder(tf.float32, shape=[batch_size, N, Z], name='xb')\n",
    "yb = tf.placeholder(tf.float32, shape=[batch_size, N * Z], name='yb')\n",
    "xb_input = tf.transpose(xb, [0, 2, 1])\n",
    "net_dict[\"LLRb{0}\".format(0)] = tf.zeros((batch_size, Z, sum_edge), dtype=tf.float32)\n",
    "for i in range(0, iters_max, 1):\n",
    "    #variable node update\n",
    "    x0 = tf.matmul(xb_input, W_skipconn2even)\n",
    "    x1 = tf.matmul(net_dict[\"LLRb{0}\".format(i)], W_odd2even)\n",
    "    x2 = tf.add(x0, x1)\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer * Z])\n",
    "    x2 = tf.matmul(x2, Lift_Matrix1[1].transpose())\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x_tile = tf.tile(x2, multiples=[1, 1, neurons_per_odd_layer])\n",
    "    W_input_reshape = tf.reshape(W_even2odd.transpose(), [-1])\n",
    "    # check node update\n",
    "    x_tile_mul = tf.multiply(x_tile, W_input_reshape)\n",
    "    x2_1 = tf.reshape(x_tile_mul, [batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer])\n",
    "    x2_abs = tf.add(tf.abs(x2_1), 100 * (1 - tf.to_float(tf.abs(x2_1) > 0)))\n",
    "    x3 = tf.reduce_min(x2_abs, axis=3)\n",
    "    x2_2 = -x2_1\n",
    "    x4 = tf.add(tf.zeros((batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer)), 1 - 2 * tf.to_float(x2_2 < 0))\n",
    "    x4_prod = -tf.reduce_prod(x4, axis=3)\n",
    "    x_output_0 = tf.multiply(x3, tf.sign(x4_prod))\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, Z * neurons_per_odd_layer])\n",
    "    x_output_0 = tf.matmul(x_output_0, Lift_Matrix2[1])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "    # add learnable parameters\n",
    "    x_output_1 = tf.add(tf.multiply(tf.abs(x_output_0), net_dict[\"Weights_Var{0}\".format(i)]), net_dict[\"Biases_Var{0}\".format(i)])\n",
    "    x_output_1 = tf.multiply(x_output_1, tf.to_float(x_output_1 > 0))\n",
    "    net_dict[\"LLRb{0}\".format(i+1)] = tf.multiply(x_output_1, tf.sign(x_output_0))\n",
    "    # output\n",
    "    y_output_2 = tf.matmul(net_dict[\"LLRb{0}\".format(i+1)], W_output)\n",
    "    y_output_3 = tf.transpose(y_output_2, [0, 2, 1])\n",
    "    y_output_4 = tf.add(xb, y_output_3)\n",
    "    net_dict[\"yb_output{0}\".format(i)] = tf.reshape(y_output_4, [batch_size, N * Z], name='yb_output'.format(i))\n",
    "    # calculate loss\n",
    "    net_dict[\"lossb{0}\".format(i)] = 1.0 * tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=yb,\n",
    "                                                            logits=net_dict[\"yb_output{0}\".format(i)]), name='lossb'.format(i))\n",
    "    # AdamOptimizer\n",
    "    net_dict[\"train_stepb{0}\".format(i)] = tf.train.AdamOptimizer(learning_rate=\n",
    "                                                                learning_rate).minimize(net_dict[\"lossb{0}\".format(i)],\n",
    "                                var_list = [net_dict[\"Weights_Var{0}\".format(i)], net_dict[\"Biases_Var{0}\".format(i)]])\n",
    "\n",
    "# the decoding neural network of Z=10\n",
    "Z = 10\n",
    "xc = tf.placeholder(tf.float32, shape=[batch_size, N, Z], name='xc')\n",
    "yc = tf.placeholder(tf.float32, shape=[batch_size, N * Z], name='yc')\n",
    "xc_input = tf.transpose(xc, [0, 2, 1])\n",
    "net_dict[\"LLRc{0}\".format(0)] = tf.zeros((batch_size, Z, sum_edge), dtype=tf.float32)\n",
    "for i in range(0, iters_max, 1):\n",
    "    # variable node update\n",
    "    x0 = tf.matmul(xc_input, W_skipconn2even)\n",
    "    x1 = tf.matmul(net_dict[\"LLRc{0}\".format(i)], W_odd2even)\n",
    "    x2 = tf.add(x0, x1)\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer * Z])\n",
    "    x2 = tf.matmul(x2, Lift_Matrix1[2].transpose())\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x_tile = tf.tile(x2, multiples=[1, 1, neurons_per_odd_layer])\n",
    "    W_input_reshape = tf.reshape(W_even2odd.transpose(), [-1])\n",
    "    # check node update\n",
    "    x_tile_mul = tf.multiply(x_tile, W_input_reshape)\n",
    "    x2_1 = tf.reshape(x_tile_mul, [batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer])\n",
    "    x2_abs = tf.add(tf.abs(x2_1), 100 * (1 - tf.to_float(tf.abs(x2_1) > 0)))\n",
    "    x3 = tf.reduce_min(x2_abs, axis=3)\n",
    "    x2_2 = -x2_1\n",
    "    x4 = tf.add(tf.zeros((batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer)), 1 - 2 * tf.to_float(x2_2 < 0))\n",
    "    x4_prod = -tf.reduce_prod(x4, axis=3)\n",
    "    x_output_0 = tf.multiply(x3, tf.sign(x4_prod))\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, Z * neurons_per_odd_layer])\n",
    "    x_output_0 = tf.matmul(x_output_0, Lift_Matrix2[2])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "    # add learnable parameters\n",
    "    x_output_1 = tf.add(tf.multiply(tf.abs(x_output_0),net_dict[\"Weights_Var{0}\".format(i)]), net_dict[\"Biases_Var{0}\".format(i)])\n",
    "    x_output_1 = tf.multiply(x_output_1, tf.to_float(x_output_1 > 0))\n",
    "    net_dict[\"LLRc{0}\".format(i+1)] = tf.multiply(x_output_1, tf.sign(x_output_0))\n",
    "    # output\n",
    "    y_output_2 = tf.matmul(net_dict[\"LLRc{0}\".format(i+1)], W_output)\n",
    "    y_output_3 = tf.transpose(y_output_2, [0, 2, 1])\n",
    "    y_output_4 = tf.add(xc, y_output_3)\n",
    "    net_dict[\"yc_output{0}\".format(i)] = tf.reshape(y_output_4, [batch_size, N * Z], name='yc_output'.format(i))\n",
    "    # calculate loss\n",
    "    net_dict[\"lossc{0}\".format(i)] = 1.0 * tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=yc,\n",
    "                                                             logits=net_dict[\"yc_output{0}\".format(i)]), name='lossc'.format(i))\n",
    "    # AdamOptimizer\n",
    "    net_dict[\"train_stepc{0}\".format(i)] = tf.train.AdamOptimizer(learning_rate=\n",
    "                                                                 learning_rate).minimize(net_dict[\"lossc{0}\".format(i)],\n",
    "                                 var_list = [net_dict[\"Weights_Var{0}\".format(i)], net_dict[\"Biases_Var{0}\".format(i)]])\n",
    "\n",
    "# the decoding neural network of Z=6\n",
    "Z = 6\n",
    "xd = tf.placeholder(tf.float32, shape=[batch_size, N, Z], name='xd')\n",
    "yd = tf.placeholder(tf.float32, shape=[batch_size, N * Z], name='yd')\n",
    "xd_input = tf.transpose(xd, [0, 2, 1])\n",
    "net_dict[\"LLRd{0}\".format(0)] = tf.zeros((batch_size, Z, sum_edge), dtype=tf.float32)\n",
    "for i in range(0, iters_max, 1):\n",
    "    # variable node update\n",
    "    x0 = tf.matmul(xd_input, W_skipconn2even)\n",
    "    x1 = tf.matmul(net_dict[\"LLRd{0}\".format(i)], W_odd2even)\n",
    "    x2 = tf.add(x0, x1)\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer * Z])\n",
    "    x2 = tf.matmul(x2, Lift_Matrix1[3].transpose())\n",
    "    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x2 = tf.transpose(x2, [0, 2, 1])\n",
    "    x_tile = tf.tile(x2, multiples=[1, 1, neurons_per_odd_layer])\n",
    "    W_input_reshape = tf.reshape(W_even2odd.transpose(), [-1])\n",
    "    # check node update\n",
    "    x_tile_mul = tf.multiply(x_tile, W_input_reshape)\n",
    "    x2_1 = tf.reshape(x_tile_mul, [batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer])\n",
    "    x2_abs = tf.add(tf.abs(x2_1), 100 * (1 - tf.to_float(tf.abs(x2_1) > 0)))\n",
    "    x3 = tf.reduce_min(x2_abs, axis=3)\n",
    "    x2_2 = -x2_1\n",
    "    x4 = tf.add(tf.zeros((batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer)),\n",
    "                1 - 2 * tf.to_float(x2_2 < 0))\n",
    "    x4_prod = -tf.reduce_prod(x4, axis=3)\n",
    "    x_output_0 = tf.multiply(x3, tf.sign(x4_prod))\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, Z * neurons_per_odd_layer])\n",
    "    x_output_0 = tf.matmul(x_output_0, Lift_Matrix2[3])\n",
    "    x_output_0 = tf.reshape(x_output_0, [batch_size, neurons_per_odd_layer, Z])\n",
    "    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n",
    "    # add learnable parameters\n",
    "    x_output_1 = tf.add(tf.multiply(tf.abs(x_output_0), net_dict[\"Weights_Var{0}\".format(i)]),\n",
    "                        net_dict[\"Biases_Var{0}\".format(i)])\n",
    "    x_output_1 = tf.multiply(x_output_1, tf.to_float(x_output_1 > 0))\n",
    "    net_dict[\"LLRd{0}\".format(i + 1)] = tf.multiply(x_output_1, tf.sign(x_output_0))\n",
    "    # output\n",
    "    y_output_2 = tf.matmul(net_dict[\"LLRd{0}\".format(i + 1)], W_output)\n",
    "    y_output_3 = tf.transpose(y_output_2, [0, 2, 1])\n",
    "    y_output_4 = tf.add(xd, y_output_3)\n",
    "    net_dict[\"yd_output{0}\".format(i)] = tf.reshape(y_output_4, [batch_size, N * Z], name='yd_output'.format(i))\n",
    "    # calculate loss\n",
    "    net_dict[\"lossd{0}\".format(i)] = 1.0 * tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=yd,\n",
    "                                                                            logits=net_dict[\"yd_output{0}\".format(i)]),\n",
    "                                                                             name='lossd'.format(i))\n",
    "    # AdamOptimizer\n",
    "    net_dict[\"train_stepd{0}\".format(i)] = tf.train.AdamOptimizer(learning_rate=\n",
    "                                                                  learning_rate).minimize(net_dict[\"lossd{0}\".format(i)],\n",
    "                                                                  var_list=[net_dict[\"Weights_Var{0}\".format(i)], net_dict[\"Biases_Var{0}\".format(i)]])\n",
    "\n",
    "\n",
    "##################################  Train  ####################################\n",
    "##################################  Train (Z = 16 only, block-wise)  ####################################\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Starting block-wise training for Z=16 ...\")\n",
    "\n",
    "Δ1 = 5    # block size\n",
    "Δ2 = 10   # retraining window\n",
    "total_iters = iters_max\n",
    "\n",
    "# Loop over blocks instead of individual iterations\n",
    "for start_iter in range(0, total_iters, Δ1):\n",
    "    end_iter = min(start_iter + Δ1, total_iters)\n",
    "    train_indices = list(range(max(0, start_iter - Δ2), end_iter))\n",
    "\n",
    "    print(\"\\n=== Training block: iters {} → {} (window: {}) ===\".format(\n",
    "        start_iter, end_iter - 1, train_indices))\n",
    "\n",
    "    for epoch in range(num_of_batch):\n",
    "        # Use only Z=16 network\n",
    "        Z = 16\n",
    "        Z_type = 0\n",
    "        SNR_set = np.array([SNR_sigma[Z_type, end_iter - 1]])\n",
    "\n",
    "        # Generate one batch\n",
    "        training_received_data, training_coded_bits = create_mix_epoch(\n",
    "            SNR_set, wordRandom, noiseRandom, numOfWordSim_train,\n",
    "            code_n, code_k, Z, Ldpc_GM[Z_type], train_on_zero_word\n",
    "        )\n",
    "\n",
    "        training_labels_for_mse = training_coded_bits\n",
    "\n",
    "        # Train all iterations in this block jointly\n",
    "        feed = {xa: training_received_data, ya: training_labels_for_mse}\n",
    "        _, block_loss = sess.run(\n",
    "            [net_dict[f\"train_stepa{train_indices[-1]}\"],\n",
    "             tf.add_n([net_dict[f\"lossa{i}\"] for i in train_indices])],\n",
    "            feed_dict=feed\n",
    "        )\n",
    "\n",
    "        # Print every few hundred iterations\n",
    "        if epoch % 200 == 0:\n",
    "            print(\"Block [{}–{}] Epoch [{}/{}]  →  Loss: {:.6f}\".format(\n",
    "                start_iter, end_iter - 1, epoch, num_of_batch, block_loss))\n",
    "\n",
    "    # ---------------- Save weights after finishing this block ----------------\n",
    "    for i in train_indices:\n",
    "        w_val, b_val = sess.run([\n",
    "            net_dict[f\"Weights_Var{i}\"],\n",
    "            net_dict[f\"Biases_Var{i}\"]\n",
    "        ])\n",
    "        np.savetxt(f'./Weights_Var/Weights_Var{i}.txt', w_val, fmt='%s', delimiter=',')\n",
    "        np.savetxt(f'./Biases_Var/Biases_Var{i}.txt', b_val, fmt='%s', delimiter=',')\n",
    "\n",
    "\n",
    "    # ##################################  save weights and biases  ####################################\n",
    "    # a, b = sess.run(fetches=[net_dict[\"Weights_Var{0}\".format(iter)], net_dict[\"Biases_Var{0}\".format(iter)]])\n",
    "    # np.savetxt('./Weights_Var/Weights_Var{0}.txt'.format(iter), a, fmt='%s', delimiter=',')\n",
    "    # np.savetxt('./Biases_Var/Biases_Var{0}.txt'.format(iter), b, fmt='%s', delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8441404,
     "sourceId": 13315891,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8441415,
     "sourceId": 13315909,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 106.29346,
   "end_time": "2025-10-09T20:46:56.707999",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-09T20:45:10.414539",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
