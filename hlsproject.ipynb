{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13315652,"sourceType":"datasetVersion","datasetId":8441256}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function, division\nimport os\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport numpy as np\nimport datetime\n\n# GPU settings\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\n# get the base graph and generator matrix\ncode_PCM0 = np.loadtxt(\"/kaggle/input/hls-project/Neural-Protograph-LDPC-Decoding-main/BaseGraph/BaseGraph2_Set0.txt\", int, delimiter='\t')\ncode_PCM1 = np.loadtxt(\"/kaggle/input/hls-project/Neural-Protograph-LDPC-Decoding-main/BaseGraph/BaseGraph2_Set1.txt\", int, delimiter='\t')\ncode_PCM2 = np.loadtxt(\"/kaggle/input/hls-project/Neural-Protograph-LDPC-Decoding-main/BaseGraph/BaseGraph2_Set2.txt\", int, delimiter='\t')\ncode_GM_16 = np.loadtxt(\"/kaggle/input/hls-project/Neural-Protograph-LDPC-Decoding-main/BaseGraph_GM/LDPC_GM_BG2_16.txt\", int, delimiter=',')\ncode_GM_3 = np.loadtxt(\"/kaggle/input/hls-project/Neural-Protograph-LDPC-Decoding-main/BaseGraph_GM/LDPC_GM_BG2_3.txt\", int, delimiter=',')\ncode_GM_10 = np.loadtxt(\"/kaggle/input/hls-project/Neural-Protograph-LDPC-Decoding-main/BaseGraph_GM/LDPC_GM_BG2_10.txt\", int, delimiter=',')\ncode_GM_6 = np.loadtxt(\"/kaggle/input/hls-project/Neural-Protograph-LDPC-Decoding-main/BaseGraph_GM/LDPC_GM_BG2_6.txt\", int, delimiter=',')\ncode_PCM = code_PCM0.copy()\nLdpc_PCM = [code_PCM0, code_PCM1, code_PCM2, code_PCM1]# four LDPC codes with different code lengths\nLdpc_GM = [code_GM_16, code_GM_3, code_GM_10, code_GM_6]\nZ_array = np.array([16, 3, 10, 6])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:38:37.670192Z","iopub.execute_input":"2025-10-09T19:38:37.670433Z","iopub.status.idle":"2025-10-09T19:38:53.141914Z","shell.execute_reply.started":"2025-10-09T19:38:37.670405Z","shell.execute_reply":"2025-10-09T19:38:53.141085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N = 52\nm = 42\ncode_n = N\ncode_k = N - m\nfor i in range(0, code_PCM.shape[0]):\n    for j in range(0, code_PCM.shape[1]):\n        if (code_PCM[i, j] == -1):\n            code_PCM[i, j] = 0\n        else:\n            code_PCM[i, j] = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:39:23.360788Z","iopub.execute_input":"2025-10-09T19:39:23.361338Z","iopub.status.idle":"2025-10-09T19:39:23.366432Z","shell.execute_reply.started":"2025-10-09T19:39:23.361289Z","shell.execute_reply":"2025-10-09T19:39:23.365743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# network hyper-parameters\niters_max = 25     # number of iterations\nsum_edge_c = np.sum(code_PCM, axis=1)\nsum_edge_v = np.sum(code_PCM, axis=0)\nsum_edge = np.sum(sum_edge_v)\nneurons_per_even_layer = neurons_per_odd_layer = np.sum(sum_edge_v)\ninput_output_layer_size = N\n\n# init the AWGN #\ncode_rate = 1.0 * (N - m) / (N-2)\n# train SNR\nSNR_Matrix = np.array([[9.0,6.05,4.1,2.95,2.25,1.8,1.55,1.3,1.15,1.05,0.94,0.85,0.83,0.81,0.8,0.8,0.8,0.75,0.75,0.7,0.7,0.7,0.7,0.7,0.7],\n                       [9.1,6.2,4.6,3.7,3.2,3.0,2.8,2.7,2.6,2.55,2.5,2.45,2.4,2.4,2.4,2.35,2.35,2.3,2.3,2.3,2.25,2.25,2.25,2.25,2.25],\n                       [9,6.05,4.1,3,2.4,2,1.7,1.5,1.4,1.4,1.3,1.3,1.2,1.2,1.2,1.2,1.1,1.1,1.1,1.1,1.1,1.1,1.1,1,1],\n                       [9.0,6.1,4.25,3.2,2.6,2.25,2,1.9,1.8,1.7,1.7,1.65,1.6,1.6,1.55,1.55,1.5,1.5,1.5,1.45,1.45,1.4,1.4,1.4,1.4]])\nSNR_lin = 10.0 ** (SNR_Matrix / 10.0)\nSNR_sigma = np.sqrt(1.0 / (2.0 * SNR_lin * code_rate))\n# ramdom seed\nword_seed = 2042\nnoise_seed = 1074\nwordRandom = np.random.RandomState(word_seed)  # word seed\nnoiseRandom = np.random.RandomState(noise_seed)  # noise seed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:39:39.464231Z","iopub.execute_input":"2025-10-09T19:39:39.464795Z","iopub.status.idle":"2025-10-09T19:39:39.472634Z","shell.execute_reply.started":"2025-10-09T19:39:39.464770Z","shell.execute_reply":"2025-10-09T19:39:39.471990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train settings\nlearning_rate = 0.001\ntrain_on_zero_word = True\nnumOfWordSim_train = 50\nbatch_size = numOfWordSim_train\nnum_of_batch = 50000\n\n#get train samples\ndef create_mix_epoch(scaling_factor, wordRandom, noiseRandom, numOfWordSim, code_n, code_k, Z, code_GM, is_zeros_word):\n    X = np.zeros([1, code_n * Z], dtype=np.float32)\n    Y = np.zeros([1, code_n * Z], dtype=np.int64)\n\n    # build set for epoch\n    for sf_i in scaling_factor:\n        if is_zeros_word:\n            infoWord_i = 0 * wordRandom.randint(0, 2, size=(numOfWordSim, code_k * Z))\n        else:\n            infoWord_i = wordRandom.randint(0, 2, size=(numOfWordSim, code_k * Z))\n\n        Y_i = np.dot(infoWord_i, code_GM) % 2\n        X_p_i = noiseRandom.normal(0.0, 1.0, Y_i.shape) * sf_i + (-1) ** (1 - Y_i)  # pay attention to this 1->1 0->-1\n        x_llr_i = 2 * X_p_i / ((sf_i) ** 2)  # defined as p1/p0\n        X = np.vstack((X, x_llr_i))\n        Y = np.vstack((Y, Y_i))\n    X = X[1:]\n    Y = Y[1:]\n    X = np.reshape(X, [batch_size, code_n, Z])\n    return X, Y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:40:31.543429Z","iopub.execute_input":"2025-10-09T19:40:31.544109Z","iopub.status.idle":"2025-10-09T19:40:31.550503Z","shell.execute_reply.started":"2025-10-09T19:40:31.544084Z","shell.execute_reply":"2025-10-09T19:40:31.549464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# calculate ber and fer\ndef calc_ber_fer(snr_db, Y_test_pred, Y_test, numOfWordSim):\n    ber_test = np.zeros(snr_db.shape[0])\n    fer_test = np.zeros(snr_db.shape[0])\n    for i in range(0, snr_db.shape[0]):\n        Y_test_pred_i = Y_test_pred[i * numOfWordSim:(i + 1) * numOfWordSim, :]\n        Y_test_i = Y_test[i * numOfWordSim:(i + 1) * numOfWordSim, :]\n        ber_test[i] = np.abs(((Y_test_pred_i > 0) - Y_test_i)).sum() / (Y_test_i.shape[0] * Y_test_i.shape[1])\n        fer_test[i] = (np.abs(((Y_test_pred_i > 0) - Y_test_i)).sum(axis=1) > 0).sum() * 1.0 / Y_test_i.shape[0]\n    return ber_test, fer_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:40:42.526399Z","iopub.execute_input":"2025-10-09T19:40:42.526985Z","iopub.status.idle":"2025-10-09T19:40:42.532147Z","shell.execute_reply.started":"2025-10-09T19:40:42.526961Z","shell.execute_reply":"2025-10-09T19:40:42.531422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"############################     init the connecting matrix between network layers   #################################\nLift_Matrix1 = []\nLift_Matrix2 = []\nW_odd2even = np.zeros((sum_edge, sum_edge), dtype=np.float32)\nW_skipconn2even = np.zeros((N, sum_edge), dtype=np.float32)\nW_even2odd = np.zeros((sum_edge, sum_edge), dtype=np.float32)\nW_output = np.zeros((sum_edge, N), dtype=np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:41:55.389952Z","iopub.execute_input":"2025-10-09T19:41:55.390759Z","iopub.status.idle":"2025-10-09T19:41:55.398884Z","shell.execute_reply.started":"2025-10-09T19:41:55.390732Z","shell.execute_reply":"2025-10-09T19:41:55.398270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init lifting matrix for cyclic shift\nfor t in range(0, 4, 1):\n    Lift_M1 = np.zeros((neurons_per_odd_layer * Z_array[t], neurons_per_odd_layer * Z_array[t]), np.float32)\n    Lift_M2 = np.zeros((neurons_per_odd_layer * Z_array[t], neurons_per_odd_layer * Z_array[t]), np.float32)\n    code_PCM1 = Ldpc_PCM[t]\n    k = 0\n    for j in range(0, code_PCM1.shape[1]):\n        for i in range(0, code_PCM1.shape[0]):\n            if (code_PCM1[i, j] != -1):\n                Lift_num = code_PCM1[i, j] % Z_array[t]\n                for h in range(0, Z_array[t], 1):\n                    Lift_M1[k * Z_array[t] + h, k * Z_array[t] + (h + Lift_num) % Z_array[t]] = 1\n                k = k + 1\n    k = 0\n    for i in range(0, code_PCM1.shape[0]):\n        for j in range(0, code_PCM1.shape[1]):\n            if (code_PCM1[i, j] != -1):\n                Lift_num = code_PCM1[i, j] % Z_array[t]\n                for h in range(0, Z_array[t], 1):\n                    Lift_M2[k * Z_array[t] + h, k * Z_array[t] + (h + Lift_num) % Z_array[t]] = 1\n                k = k + 1\n    Lift_Matrix1.append(Lift_M1)\n    Lift_Matrix2.append(Lift_M2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:41:55.737097Z","iopub.execute_input":"2025-10-09T19:41:55.737370Z","iopub.status.idle":"2025-10-09T19:41:55.801399Z","shell.execute_reply.started":"2025-10-09T19:41:55.737350Z","shell.execute_reply":"2025-10-09T19:41:55.800814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init W_odd2even  variable node updating\nk = 0\nvec_tmp = np.zeros((sum_edge), dtype=np.float32)  # even layer index read with column\nfor j in range(0, code_PCM.shape[1], 1):  # run over the columns\n    for i in range(0, code_PCM.shape[0], 1):  # break after the first one\n        if (code_PCM[i, j] == 1):  # finding the first one is ok\n            num_of_conn = int(np.sum(code_PCM[:, j]))  # get the number of connection of the variable node\n            idx = np.argwhere(code_PCM[:, j] == 1)  # get the indexes\n            for l in range(0, num_of_conn, 1):  # adding num_of_conn columns to W\n                vec_tmp = np.zeros((sum_edge), dtype=np.float32)\n                for r in range(0, code_PCM.shape[0], 1):  # adding one to the right place\n                    if (code_PCM[r, j] == 1 and idx[l][0] != r):\n                        idx_row = np.cumsum(code_PCM[r, 0:j + 1])[-1] - 1\n                        odd_layer_node_count = 0\n                        if r > 0:\n                            odd_layer_node_count = np.cumsum(sum_edge_c[0:r])[-1]\n                        vec_tmp[idx_row + odd_layer_node_count] = 1  # offset index adding\n                W_odd2even[:, k] = vec_tmp.transpose()\n                k += 1\n            break\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:41:57.031021Z","iopub.execute_input":"2025-10-09T19:41:57.031535Z","iopub.status.idle":"2025-10-09T19:41:57.056251Z","shell.execute_reply.started":"2025-10-09T19:41:57.031510Z","shell.execute_reply":"2025-10-09T19:41:57.055482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init W_even2odd  parity check node updating\nk = 0\nfor j in range(0, code_PCM.shape[1], 1):\n    for i in range(0, code_PCM.shape[0], 1):\n        if (code_PCM[i, j] == 1):\n            idx_row = np.cumsum(code_PCM[i, 0:j + 1])[-1] - 1\n            idx_col = np.cumsum(code_PCM[0: i + 1, j])[-1] - 1\n            odd_layer_node_count_1 = 0\n            odd_layer_node_count_2 = np.cumsum(sum_edge_c[0:i + 1])[-1]\n            if i > 0:\n                odd_layer_node_count_1 = np.cumsum(sum_edge_c[0:i])[-1]\n            W_even2odd[k, odd_layer_node_count_1:odd_layer_node_count_2] = 1.0\n            W_even2odd[k, odd_layer_node_count_1 + idx_row] = 0.0\n            k += 1  # k is counted in column direction\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:41:59.745553Z","iopub.execute_input":"2025-10-09T19:41:59.745822Z","iopub.status.idle":"2025-10-09T19:41:59.753959Z","shell.execute_reply.started":"2025-10-09T19:41:59.745803Z","shell.execute_reply":"2025-10-09T19:41:59.753281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init W_output odd to output\nk = 0\nfor j in range(0, code_PCM.shape[1], 1):\n    for i in range(0, code_PCM.shape[0], 1):\n        if (code_PCM[i, j] == 1):\n            idx_row = np.cumsum(code_PCM[i, 0:j + 1])[-1] - 1\n            idx_col = np.cumsum(code_PCM[0: i + 1, j])[-1] - 1\n            odd_layer_node_count = 0\n            if i > 0:\n                odd_layer_node_count = np.cumsum(sum_edge_c[0:i])[-1]\n            W_output[odd_layer_node_count + idx_row, k] = 1.0\n    k += 1\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:42:01.852505Z","iopub.execute_input":"2025-10-09T19:42:01.852777Z","iopub.status.idle":"2025-10-09T19:42:01.860243Z","shell.execute_reply.started":"2025-10-09T19:42:01.852759Z","shell.execute_reply":"2025-10-09T19:42:01.859535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# init W_skipconn2even  channel input\nk = 0\nfor j in range(0, code_PCM.shape[1], 1):\n    for i in range(0, code_PCM.shape[0], 1):\n        if (code_PCM[i, j] == 1):\n            W_skipconn2even[j, k] = 1.0\n            k += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:42:02.925714Z","iopub.execute_input":"2025-10-09T19:42:02.925990Z","iopub.status.idle":"2025-10-09T19:42:02.931052Z","shell.execute_reply.started":"2025-10-09T19:42:02.925969Z","shell.execute_reply":"2025-10-09T19:42:02.930231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##############################  bulid neural networks ############################\nnet_dict = {}\n# init the learnable network parameters\nWeights_Var = 0.5 * np.ones(sum_edge, dtype=np.float32)\nBiases_Var = np.zeros(sum_edge, dtype=np.float32)\nfor i in range(0, iters_max, 1):\n    net_dict[\"Weights_Var{0}\".format(i)] = tf.Variable(Weights_Var.copy(), name=\"Weights_Var\".format(i))\n    net_dict[\"Biases_Var{0}\".format(i)] = tf.Variable(Biases_Var.copy(), name=\"Biases_Var\".format(i))\n\n# the decoding neural network of Z=16\nZ = 16\nxa = tf.placeholder(tf.float32, shape=[batch_size, N, Z], name='xa')\nya = tf.placeholder(tf.float32, shape=[batch_size, N * Z], name='ya')\nxa_input = tf.transpose(xa, [0, 2, 1])\nnet_dict[\"LLRa{0}\".format(0)] = tf.zeros((batch_size, Z, sum_edge), dtype=tf.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:42:19.351391Z","iopub.execute_input":"2025-10-09T19:42:19.351662Z","iopub.status.idle":"2025-10-09T19:42:19.480429Z","shell.execute_reply.started":"2025-10-09T19:42:19.351641Z","shell.execute_reply":"2025-10-09T19:42:19.479869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport os\n\nos.makedirs('/kaggle/working/Weights_Var', exist_ok=True)\nos.makedirs('/kaggle/working/Biases_Var', exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:55:03.449219Z","iopub.execute_input":"2025-10-09T19:55:03.449822Z","iopub.status.idle":"2025-10-09T19:55:03.453028Z","shell.execute_reply.started":"2025-10-09T19:55:03.449801Z","shell.execute_reply":"2025-10-09T19:55:03.452444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(0, iters_max, 1):\n    #variable node update\n    x0 = tf.matmul(xa_input, W_skipconn2even)\n    x1 = tf.matmul(net_dict[\"LLRa{0}\".format(i)], W_odd2even)\n    x2 = tf.add(x0, x1)\n    x2 = tf.transpose(x2, [0, 2, 1])\n    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer * Z])\n    x2 = tf.matmul(x2, Lift_Matrix1[0].transpose())\n    x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer, Z])\n    x2 = tf.transpose(x2, [0, 2, 1])\n    x_tile = tf.tile(x2, multiples=[1, 1, neurons_per_odd_layer])\n    W_input_reshape = tf.reshape(W_even2odd.transpose(), [-1])\n    #check node update\n    x_tile_mul = tf.multiply(x_tile, W_input_reshape)\n    x2_1 = tf.reshape(x_tile_mul, [batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer])\n    x2_abs = tf.add(tf.abs(x2_1), 10000 * (1 - tf.to_float(tf.abs(x2_1) > 0)))\n    x3 = tf.reduce_min(x2_abs, axis=3)\n    x2_2 = -x2_1\n    x4 = tf.add(tf.zeros((batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer)), 1 - 2 * tf.to_float(x2_2 < 0))\n    x4_prod = -tf.reduce_prod(x4, axis=3)\n    x_output_0 = tf.multiply(x3, tf.sign(x4_prod))\n    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n    x_output_0 = tf.reshape(x_output_0, [batch_size, Z * neurons_per_odd_layer])\n    x_output_0 = tf.matmul(x_output_0, Lift_Matrix2[0])\n    x_output_0 = tf.reshape(x_output_0, [batch_size, neurons_per_odd_layer, Z])\n    x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n    # add learnable parameters\n        # ===================== Dynamic Weight Sharing (DWS) ======================\n    \n    # Step 1: Detect satisfied / unsatisfied check nodes\n    # Compute parity satisfaction using the sign product\n    CN_sign = tf.sign(x_output_0)  # sign of messages from each CN\n    CN_satisfied = tf.cast(tf.equal(tf.reduce_prod(CN_sign, axis=2), 1), tf.float32)  # parity satisfied -> 1\n    CN_unsatisfied = 1.0 - CN_satisfied  # parity unsatisfied -> 1\n    \n    # Step 2: Define dynamic weights for SCNs and UCNs per iteration\n    net_dict[\"Weights_SCN{0}\".format(i)] = tf.Variable(\n        0.5 * np.ones(neurons_per_odd_layer, dtype=np.float32),\n        name=\"Weights_SCN{0}\".format(i))\n    net_dict[\"Weights_UCN{0}\".format(i)] = tf.Variable(\n        0.5 * np.ones(neurons_per_odd_layer, dtype=np.float32),\n        name=\"Weights_UCN{0}\".format(i))\n    \n    # Step 3: Apply corresponding weights dynamically\n    # |x_output_0| gives magnitude of CN messages\n    x_output_SCN = tf.multiply(tf.abs(x_output_0), net_dict[\"Weights_SCN{0}\".format(i)])\n    x_output_UCN = tf.multiply(tf.abs(x_output_0), net_dict[\"Weights_UCN{0}\".format(i)])\n    \n    # broadcast CN_satisfied/unsatisfied along last dimension (Z axis)\n    x_output_1 = (x_output_SCN * tf.expand_dims(CN_satisfied, -1) +\n                  x_output_UCN * tf.expand_dims(CN_unsatisfied, -1))\n    \n    # Add bias as before\n    x_output_1 = tf.add(x_output_1, net_dict[\"Biases_Var{0}\".format(i)])\n    \n    # ReLU nonlinearity\n    x_output_1 = tf.multiply(x_output_1, tf.to_float(x_output_1 > 0))\n    \n    # Update LLR for next iteration\n    net_dict[\"LLRa{0}\".format(i+1)] = tf.multiply(x_output_1, tf.sign(x_output_0))\n    # ========================================================================\n\n    # output\n    y_output_2 = tf.matmul(net_dict[\"LLRa{0}\".format(i+1)], W_output)\n    y_output_3 = tf.transpose(y_output_2, [0, 2, 1])\n    y_output_4 = tf.add(xa, y_output_3)\n    net_dict[\"ya_output{0}\".format(i)] = tf.reshape(y_output_4, [batch_size, N * Z], name='ya_output'.format(i))\n    # calculate loss\n    net_dict[\"lossa{0}\".format(i)] = 1.0 * tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=ya,\n                                                            logits=net_dict[\"ya_output{0}\".format(i)]), name='lossa'.format(i))\n    # AdamOptimizer\n    net_dict[\"train_stepa{0}\".format(i)] = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n    net_dict[\"lossa{0}\".format(i)],\n    var_list=[net_dict[\"Weights_SCN{0}\".format(i)],\n              net_dict[\"Weights_UCN{0}\".format(i)],\n              net_dict[\"Biases_Var{0}\".format(i)]])\n\n\n\n##################################  Train  ####################################\nsess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:55:34.328509Z","iopub.execute_input":"2025-10-09T19:55:34.329005Z","iopub.status.idle":"2025-10-09T19:56:19.347176Z","shell.execute_reply.started":"2025-10-09T19:55:34.328983Z","shell.execute_reply":"2025-10-09T19:56:19.346609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# for i in range(0, iters_max, 1):\n#     #variable node update\n#     x0 = tf.matmul(xa_input, W_skipconn2even)\n#     x1 = tf.matmul(net_dict[\"LLRa{0}\".format(i)], W_odd2even)\n#     x2 = tf.add(x0, x1)\n#     x2 = tf.transpose(x2, [0, 2, 1])\n#     x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer * Z])\n#     x2 = tf.matmul(x2, Lift_Matrix1[0].transpose())\n#     x2 = tf.reshape(x2, [batch_size, neurons_per_odd_layer, Z])\n#     x2 = tf.transpose(x2, [0, 2, 1])\n#     x_tile = tf.tile(x2, multiples=[1, 1, neurons_per_odd_layer])\n#     W_input_reshape = tf.reshape(W_even2odd.transpose(), [-1])\n#     #check node update\n#     x_tile_mul = tf.multiply(x_tile, W_input_reshape)\n#     x2_1 = tf.reshape(x_tile_mul, [batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer])\n#     x2_abs = tf.add(tf.abs(x2_1), 10000 * (1 - tf.to_float(tf.abs(x2_1) > 0)))\n#     x3 = tf.reduce_min(x2_abs, axis=3)\n#     x2_2 = -x2_1\n#     x4 = tf.add(tf.zeros((batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer)), 1 - 2 * tf.to_float(x2_2 < 0))\n#     x4_prod = -tf.reduce_prod(x4, axis=3)\n#     x_output_0 = tf.multiply(x3, tf.sign(x4_prod))\n#     x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n#     x_output_0 = tf.reshape(x_output_0, [batch_size, Z * neurons_per_odd_layer])\n#     x_output_0 = tf.matmul(x_output_0, Lift_Matrix2[0])\n#     x_output_0 = tf.reshape(x_output_0, [batch_size, neurons_per_odd_layer, Z])\n#     x_output_0 = tf.transpose(x_output_0, [0, 2, 1])\n#     # ===================== Dynamic Weight Sharing (DWS) - CORRECT ======================\n\n#     # At this point x_output_0 has shape [batch_size, Z, neurons_per_odd_layer]\n#     # We'll compute the per-edge UCN mask using VN APP signs (same logic as Main_Functions).\n    \n#     # --- 1) Compute current APP for variable nodes (same as original code's y_output_4)\n#     # Note: y_output_4 is computed later in your original code; we can compute temporary VN_APP\n#     # from current LLRs: current APP = xa + sum(C->V messages)\n#     # We already have x_output_0 -> next LLRs are computed after weighting; but to follow Main_Functions\n#     # we use the previous iteration's APP. To be consistent, use the current y_output_4 (which is computed below in original).\n#     # For safety here, compute VN_APP from available tensors:\n#     # Build tentative APP as in original: sum incoming C->V (x_output_0 with sign) plus xa_input\n#     temp_LLR = tf.multiply(tf.abs(x_output_0), tf.sign(x_output_0))  # this is |msg| * sign = original msg (shape [B,Z,E])\n#     # Sum C->V into VN domain via Lift_Matrix2 and W_output path:\n#     # reshape-> matmul by Lift_Matrix2 -> reshape -> sum with W_output\n#     tmp = tf.transpose(temp_LLR, [0, 2, 1])                         # [B, E, Z]\n#     tmp = tf.reshape(tmp, [batch_size, Z * neurons_per_odd_layer])   # [B, Z*E]\n#     tmp = tf.matmul(tmp, Lift_Matrix2[0])                            # permute -> [B, Z*E]\n#     tmp = tf.reshape(tmp, [batch_size, neurons_per_odd_layer, Z])    # [B, E, Z]\n#     tmp = tf.transpose(tmp, [0, 2, 1])                               # [B, Z, E]\n#     y_tmp = tf.matmul(tmp, W_output)                                 # [B, Z, N]  (since W_output was [E, N])\n#     y_tmp = tf.transpose(y_tmp, [0, 2, 1])                           # [B, N, Z]\n#     VN_APP = tf.add(xa, y_tmp)                                       # [B, N, Z]  tentative APP\n    \n#     # Follow Main_Functions path to compute per-edge UCN mask from VN_APP\n#     VN_APP_neg = -VN_APP                              # same sign convention as Main_Functions\n#     VN_APP_sign = tf.add(tf.to_float(VN_APP_neg > 0), -tf.to_float(VN_APP_neg <= 0))  # [B,N,Z] in {+1,-1}\n    \n#     # Map VN signs to edges\n#     VN_APP_sign_edge = tf.matmul(VN_APP_sign, W_skipconn2even)   # [B, N, Z] x [N, E] -> [B, Z, E(V)] (thanks to numpy matmul broadcasting)\n#     VN_APP_sign_edge = tf.transpose(VN_APP_sign_edge, [0, 2, 1])  # [B, E, Z]\n#     VN_APP_sign_edge = tf.reshape(VN_APP_sign_edge, [batch_size, neurons_per_odd_layer * Z])  # [B, E*Z]\n    \n#     # Permute to CN inputs\n#     CN_in_sign = tf.matmul(VN_APP_sign_edge, Lift_Matrix1[0].transpose())   # [B, E*Z]\n#     CN_in_sign = tf.reshape(CN_in_sign, [batch_size, neurons_per_odd_layer, Z])  # [B, E, Z]\n#     CN_in_sign = tf.transpose(CN_in_sign, [0, 2, 1])  # [B, Z, E]\n    \n#     # Build tile and compute sign product across edges of each CN using adjacency (same trick)\n#     CN_in_sign_tile = tf.tile(CN_in_sign, multiples=[1, 1, neurons_per_odd_layer])  # [B,Z,E,E]\n#     CN_in_sign_tile = tf.multiply(CN_in_sign_tile, tf.reshape(W_even2odd_with_self.transpose(), [-1]))\n#     CN_in_sign_tile = tf.reshape(CN_in_sign_tile, [batch_size, Z, neurons_per_odd_layer, neurons_per_odd_layer])\n#     CN_in_sign_tile = tf.add(CN_in_sign_tile, 1.0 * (1 - tf.to_float(tf.abs(CN_in_sign_tile) > 0)))  # avoid zeros\n#     CN_sign_edge = tf.reduce_prod(CN_in_sign_tile, axis=3)  # [B,Z,E] product across neighbors -> sign per CN-edge\n#     UCN_idx_edge = tf.to_float(CN_sign_edge < 0)            # [B,Z,E] 1 for unsatisfied, 0 for satisfied\n    \n#     # Convert shape to match message tensors ordering (we already have [B,Z,E])\n#     SCN_idx_edge = 1.0 - UCN_idx_edge                       # satisfied mask\n    \n#     # --- 2) Define DWS trainable weights (per edge dimension) for this iteration\n#     # Use same shape as original Weights_Var / Biases (E elements)\n#     w_scn_name = \"Weights_SCN{0}\".format(i)\n#     w_ucn_name = \"Weights_UCN{0}\".format(i)\n#     bias_name  = \"Biases_Var{0}\".format(i)\n    \n#     # create variables if not already in net_dict\n#     if w_scn_name not in net_dict:\n#         net_dict[w_scn_name] = tf.Variable(0.5 * np.ones(neurons_per_odd_layer, dtype=np.float32), name=w_scn_name)\n#     if w_ucn_name not in net_dict:\n#         net_dict[w_ucn_name] = tf.Variable(0.5 * np.ones(neurons_per_odd_layer, dtype=np.float32), name=w_ucn_name)\n#     # Bias already exists in your code as Biases_Var{i}\n    \n#     # --- 3) Apply weights per-edge using masks\n#     # x_output_0 is [B, Z, E]\n#     x_abs = tf.abs(x_output_0)  # [B, Z, E]\n#     W_scn = net_dict[w_scn_name]  # shape [E]\n#     W_ucn = net_dict[w_ucn_name]\n    \n#     # reshape weights to broadcast: [1,1,E]\n#     W_scn_b = tf.reshape(W_scn, [1, 1, -1])\n#     W_ucn_b = tf.reshape(W_ucn, [1, 1, -1])\n    \n#     x_output_SCN = x_abs * W_scn_b\n#     x_output_UCN = x_abs * W_ucn_b\n    \n#     # Apply masks\n#     x_output_1 = x_output_SCN * SCN_idx_edge + x_output_UCN * UCN_idx_edge\n    \n#     # Add bias (bias vector has length E, broadcast to [B,Z,E])\n#     bias_b = tf.reshape(net_dict[bias_name], [1, 1, -1])\n#     x_output_1 = x_output_1 + bias_b\n    \n#     # ReLU and restore sign\n#     x_output_1 = x_output_1 * tf.to_float(x_output_1 > 0)\n#     net_dict[\"LLRa{0}\".format(i+1)] = x_output_1 * tf.sign(x_output_0)\n    \n#     # ========================================================================\n\n#     # output\n#     y_output_2 = tf.matmul(net_dict[\"LLRa{0}\".format(i+1)], W_output)\n#     y_output_3 = tf.transpose(y_output_2, [0, 2, 1])\n#     y_output_4 = tf.add(xa, y_output_3)\n#     net_dict[\"ya_output{0}\".format(i)] = tf.reshape(y_output_4, [batch_size, N * Z], name='ya_output'.format(i))\n#     # calculate loss\n#     net_dict[\"lossa{0}\".format(i)] = 1.0 * tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=ya,\n#                                                             logits=net_dict[\"ya_output{0}\".format(i)]), name='lossa'.format(i))\n#     # AdamOptimizer\n#     net_dict[\"train_stepa{0}\".format(i)] = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n#     net_dict[\"lossa{0}\".format(i)],\n#     var_list=[net_dict[\"Weights_SCN{0}\".format(i)],\n#               net_dict[\"Weights_UCN{0}\".format(i)],\n#               net_dict[\"Biases_Var{0}\".format(i)]])\n\n\n\n# #################################  Train  ####################################\n# sess.run(tf.global_variables_initializer())\n# saver = tf.train.Saver()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-10-09T20:02:41.633430Z","iopub.execute_input":"2025-10-09T20:02:41.633702Z","iopub.status.idle":"2025-10-09T20:02:42.490985Z","shell.execute_reply.started":"2025-10-09T20:02:41.633684Z","shell.execute_reply":"2025-10-09T20:02:42.490017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for iter in range(0, iters_max, 1):\n    for i in range(0, num_of_batch, 1):\n        #ramdom choose Z with different probabilities\n        index = [0, 1, 2, 3, 0, 0]\n        Z = 16\n        SNR_set = np.array([SNR_sigma[0, iter]])\n        training_received_data, training_coded_bits = create_mix_epoch(SNR_set, wordRandom, noiseRandom, numOfWordSim_train,\n                                                                        code_n, code_k, Z,\n                                                                        Ldpc_GM[0],\n                                                                        train_on_zero_word)\n        training_labels_for_mse = training_coded_bits\n        y_pred, train_loss, _ = sess.run(fetches=[net_dict[\"ya_output{0}\".format(iter)], net_dict[\"lossa{0}\".format(iter)],\n                                                    net_dict[\"train_stepa{0}\".format(iter)]],\n                                            feed_dict={xa: training_received_data, ya: training_labels_for_mse})\n        if i % 200 == 0:\n            print('iteration: [{0}/{1}]\\t'\n                    'epoch: [{2}/{3}]\\t'\n                    'loss: {4}\\t'.format(\n                iter + 1, iters_max, i, num_of_batch, train_loss))\n\n    ##################################  save weights and biases  ####################################\n    a, b = sess.run(fetches=[net_dict[\"Weights_Var{0}\".format(iter)], net_dict[\"Biases_Var{0}\".format(iter)]])\n    np.savetxt('/kaggle/working/Weights_Var/Weights_Var{0}.txt'.format(iter), a, fmt='%s', delimiter=',')\n    np.savetxt('/kaggle/working/Biases_Var/Biases_Var{0}.txt'.format(iter), b, fmt='%s', delimiter=',')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T19:56:19.355541Z","iopub.execute_input":"2025-10-09T19:56:19.355706Z","iopub.status.idle":"2025-10-09T20:02:36.953253Z","shell.execute_reply.started":"2025-10-09T19:56:19.355693Z","shell.execute_reply":"2025-10-09T20:02:36.952266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}